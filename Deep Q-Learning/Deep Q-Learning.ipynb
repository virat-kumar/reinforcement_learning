{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "### Background \n",
    "  \n",
    "  \n",
    "Q-Learning algorithms does a good job in small environment games where the Q-Value pairs are numbers of smaller magnitude but, as the complexity of environment inceases lets say we  want an agent to play a mario or games like break-out the observation space will be 700 * 700 * 3 * action space shape which is quite a large number so the greedy approch of Q-Learning is very inefficient way to train an agent to perform something good. To overcome this limitation we go for another kind of learning algorithm called Deep Q-Learning algorithm.\n",
    "  \n",
    "  \n",
    "### Intution\n",
    "  \n",
    "  \n",
    "We use artificial neural network to estimate the Q-value for each state action pair in the given environment. This new estimator the neural network does the work of bellman equation which we had used previously to find the optimal Q-value of a action of a given state.\n",
    "The input to the neural network is the image of the environment, it does its mathematical calculation, comes out with a q-value or the reward it would expect after the action is performed, then we get the real reward from the environment and then the loss is calculated and backpropogated to the network. This set of acions are repeated iteratively where the neural network starts to gradually learn about about the states of agent and comes up with a good policy.\n",
    "  \n",
    "  \n",
    "#### Preprocessing \n",
    "\n",
    "The image or the pixel set form the environment is converted to grayscale image format so that the model is less computational for faster training of the model.  \n",
    "\n",
    "\n",
    "#### Training\n",
    "  \n",
    "  \n",
    "Instead of passing single image as an input to the model we use set of previous images stacked over each other as an input, this is done so that the model can be able to distinguish some key features such  as speed of ball, momentum of the agent etc.\n",
    "A reward is predicted by the model and then we have the real reward from the environment.\n",
    "  \n",
    "The loss is calculated and backpropogated to the network we can use sochastic gradient decent or the adam optimizers to train our model.\n",
    "\n",
    "#### Installation\n",
    "`pip install gym[atari]`\n",
    "  \n",
    "  \n",
    "  \n",
    "### Experience reply\n",
    "  \n",
    "With experience reply we store the agents experience over several episodes at each time step in a data structure called **replay memory**. Here we do need to specify some finite episode number for which we want replay meomory. Each experience at a time step is represented by $e_t$ and defined as \n",
    "  \n",
    "$e_t=(s_t,a_t,r_{t+1},s_{t+1})$  \n",
    "  \n",
    "Where,\n",
    "  \n",
    "$s_t$ is the current state of agent  \n",
    "$a_t$ is the action taken by agent  \n",
    "$r_{t+1}$ is the reward the agent gets  \n",
    "$s_{t+1}$ is the new state the agent achieves  \n",
    "\n",
    "**We choose some random time set from the replay memory rather than all of them** why?  \n",
    "They key reason is to break the corelation between consecutive sample, If the nework learns  only from the consecutive samples of experiences as they occur in the environment the samples will be highly corelated and therefore lead to inefficient learning. Taking random breaks this corelation.\n",
    "  \n",
    "Initially :\n",
    "* N as the size of replay memory.\n",
    "* We initialize the nework with random weights.\n",
    "* For each episode we define the starting state of the episode.\n",
    "* Strategy : We either take a random action to explore the environment or we take a greedy \n",
    "approch to move further in environment.\n",
    "* Action is selected.\n",
    "* Observe the reward for the action and also the next state of the agent.\n",
    "* Store the entire tuple i.e the current state, action taken, reward received, next state\n",
    "in the replay memory\n",
    "* Random sample a batch of experience form the replay memory.\n",
    "* \n",
    "**Lets play Breakout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
